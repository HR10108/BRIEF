# ğŸ¯ BRIEF: Bi-level Coreset Selection for Efficient Instruction Tuning in LLMs

<div align="center">

![Framework](framework.png)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

*ğŸ”¬ A novel bi-level optimization framework for efficient coreset selection in Large Language Model instruction tuning*

</div>

## ğŸ“‹ Overview

**BRIEF** addresses the challenge of efficient instruction tuning by selecting high-quality subsets (coresets) of instruction examples that maintain model performance while significantly reducing training costs. Our approach makes three key contributions:

**ğŸ”¬ Theoretical Foundation**: We prove that SFT loss naturally decomposes into two fundamental components:
- ğŸ§  **Knowledge-related capability** (KN): Real-world knowledge generation
- ğŸ“ **Instruction following capability** (IF): Task instruction execution

**ğŸ¯ Novel Optimization Objective**: Unlike traditional coreset methods that minimize the overall gradient approximation error, we propose minimizing the sum of individual KN and IF gradient errors to preserve both capabilities simultaneously.

**ğŸ› ï¸ Bi-level Optimization Framework**: We design an efficient algorithm that solves this reformulated optimization problem through ternary search and submodular maximization, achieving **3Ã— training speedup** and **5% accuracy improvement**.

## âš¡ Key Features

- ğŸ¯ **Dual-capability preservation**: Maintains both knowledge and instruction-following abilities
- ğŸš€ **3Ã— computational reduction**: Significant training efficiency improvements
- ğŸ”¬ **Theoretical foundation**: Mathematically proven gradient approximation bounds
- ğŸ› ï¸ **Bi-level optimization**: Efficient solution with provable guarantees

## ğŸ—ï¸ Architecture

Our framework operates through a sophisticated bi-level optimization process:

1. **ğŸ”¥ Warm-up Phase**: Initial model training on sampled data
2. **ğŸ“Š Gradient Extraction**: Decompose gradients into KN and IF components  
3. **ğŸ¯ Bi-level Optimization**: 
   - *Upper level*: Ternary search for optimal solution space partitioning (Î±)
   - *Lower level*: Submodular maximization for coreset selection
4. **ğŸ“ Final Training**: Train on selected coreset with full efficiency

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install dependencies
pip install -r requirements.txt

# Ensure GPU availability
nvidia-smi
```

### ğŸ”§ Configuration

Edit `script/default_config.sh` to set your paths and parameters:

```bash
# Model and data paths
MODEL_TRAIN_PATH="../cache/Llama3-8B-Base"
DATA_FILE="./data/tulu3_mix.jsonl"

# Training parameters  
MAX_LENGTH=2048
PROPORTIONS="0.05"  # 5% coreset size
```

### ğŸ“ Usage Pipeline

Execute the complete BRIEF pipeline with these four steps:

```bash
# Step 1: ğŸ”¥ Warm-up training (5% sample)
bash script/step1_warm_up.sh script/default_config.sh

# Step 2: ğŸ“Š Extract gradients for KN/IF decomposition  
bash script/step2_get_gradients.sh script/default_config.sh

# Step 3: ğŸ¯ BRIEF coreset selection with bi-level optimization
bash script/step3_BRIEF.sh script/default_config.sh

# Step 4: ğŸ“ Final training on selected coreset
bash script/step4_train.sh script/default_config.sh
```

### âš™ï¸ Advanced Configuration

Customize BRIEF selection parameters in `default_config.sh`:

```bash
# Bi-level optimization settings
ENABLE_AUTO_SEARCH="true"      # Automatic Î± (partitioning parameter) search
SEARCH_PRECISION="0.005"       # Search precision threshold
MAX_ITERATIONS="15"            # Maximum search iterations

# Computational settings
DEVICE="cpu"                   # or "cuda" for GPU acceleration
BATCH_SIZE=32                  # Processing batch size
GRAD_TYPE="unormalized"        # Gradient normalization type
```

## ğŸ“ Project Structure

```
BRIEF/
â”œâ”€â”€ ğŸ“œ script/                    # ğŸ¯ Main execution scripts
â”‚   â”œâ”€â”€ default_config.sh        # âš™ï¸ Configuration settings
â”‚   â”œâ”€â”€ step1_warm_up.sh         # ğŸ”¥ Warm-up training
â”‚   â”œâ”€â”€ step2_get_gradients.sh   # ğŸ“Š Gradient extraction  
â”‚   â”œâ”€â”€ step3_BRIEF.sh          # ğŸ¯ Coreset selection
â”‚   â””â”€â”€ step4_train.sh          # ğŸ“ Final training
â”œâ”€â”€ ğŸ§® brief/                    # Core implementation
â”‚   â”œâ”€â”€ coreset/                # ğŸ¯ BRIEF algorithm
â”‚   â”œâ”€â”€ get_grads/             # ğŸ“Š Gradient computation
â”‚   â””â”€â”€ train/                 # ğŸ“ Training modules
â”œâ”€â”€ ğŸ› ï¸ tools/                   # Utility scripts
â”œâ”€â”€ ğŸ–¼ï¸ framework.png            # Architecture visualization
â””â”€â”€ ğŸ“‹ requirements.txt         # Dependencies
```

## ğŸ”¬ Theoretical Foundation

### Mathematical Decomposition

BRIEF is built on the theoretical insight that SFT loss naturally decomposes:

```
L_SFT = L_KN + L_IF
```

Where:
- `L_KN`: Knowledge-related loss component
- `L_IF`: Instruction following loss component

### Optimization Objective  

Unlike traditional methods minimizing `||E_{L_SFT}||`, BRIEF optimizes:

```
min Î£(||E_{L_KN}|| + ||E_{L_IF}||)
```

This ensures both capabilities are simultaneously preserved with bounded approximation error.


## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


---